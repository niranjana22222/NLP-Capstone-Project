{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Capstone Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# load COCO metadata\n",
    "filename = get_data_path(\"captions_train2014.json\")\n",
    "with Path(filename).open() as f:\n",
    "    coco_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "from gensim.models import KeyedVectors\n",
    "filename = \"glove.6B.200d.txt.w2v\"\n",
    "\n",
    "# this takes a while to load -- keep this in mind when designing your capstone project\n",
    "glove = KeyedVectors.load_word2vec_format(get_data_path(filename), binary=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_regex = re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "\n",
    "\n",
    "filename = \"glove.6B.200d.txt.w2v\"\n",
    "glove = KeyedVectors.load_word2vec_format(get_data_path(filename), binary=False)\n",
    "\n",
    "captions = [ann['caption'] for ann in coco_data['annotations']]\n",
    "all_words = [word for caption in captions for word in caption.split()]\n",
    "word_counts = Counter(all_words)\n",
    "num_docs = len(captions)\n",
    "idf = {}\n",
    "for word, count in word_counts.items():\n",
    "    idf[word] = np.log(num_docs / (count + 1)) \n",
    "\n",
    "def embed_caption(caption, glove, idf):\n",
    "    tokens = [punc_regex.sub('', w.lower()) for w in caption.split()] # Remove punctuation\n",
    "    real_embeddings = []\n",
    "    for word in tokens:\n",
    "        if word in glove:\n",
    "            embedding = glove[word]\n",
    "            weight = idf.get(word, 0)\n",
    "            real_embeddings.append(embedding * weight)\n",
    "    if real_embeddings:\n",
    "        embedding_sum = np.sum(real_embeddings, axis=0)\n",
    "        return embedding_sum / np.linalg.norm(embedding_sum)\n",
    "    else:\n",
    "        return np.zeros(200)\n",
    "\n",
    "\n",
    "all_captions = [caption_info[\"caption\"] for caption_info in coco_data[\"annotations\"]]\n",
    "\n",
    "all_embeddings = [embed_caption(caption,glove,idf) for caption in all_captions]\n",
    "\n",
    "final_embeddings = {}\n",
    "ids = [caption_info[\"image_id\"] for caption_info in coco_data[\"annotations\"]]\n",
    "\n",
    "for i in np.arange(len(all_embeddings)):\n",
    "    final_embeddings[ids[i]] = all_embeddings[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved image descriptor vectors\n",
    "import pickle\n",
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "with Path(get_data_path('resnet18_features.pkl')).open('rb') as f:\n",
    "    resnet18_features = pickle.load(f)\n",
    "\n",
    "#print(resnet18_features.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad as mg\n",
    "import mynn\n",
    "import numpy as np\n",
    "\n",
    "from mygrad.nnet.initializers import he_normal\n",
    "from mygrad.nnet import margin_ranking_loss\n",
    "from mynn.layers.dense import dense\n",
    "from mynn.losses.mean_squared_loss import mean_squared_loss\n",
    "from mynn.optimizers.sgd import SGD\n",
    "from mynn.optimizers.adam import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.initializers.he_normal import he_normal\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\" This initializes all of the layers in our model, and sets them\n",
    "        as attributes of the model.\n",
    "       \n",
    "        \"\"\"\n",
    "        self.M = he_normal(512, 200)\n",
    "        self.b = he_normal(1, 200)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        '''Passes data as input to our model, performing a \"forward-pass\".\n",
    "        \n",
    "        This allows us to conveniently initialize a model `m` and then send data through it\n",
    "        to be classified by calling `m(x)`.\n",
    "        \n",
    "        '''\n",
    "        return x @ self.M + self.b\n",
    "        \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model \"\"\"\n",
    "       \n",
    "        return (self.M, self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# load COCO metadata\n",
    "filename = get_data_path(\"captions_train2014.json\")\n",
    "with Path(filename).open() as f:\n",
    "    coco_data = json.load(f)\n",
    "    \n",
    "#set(resnet18_features) < set(img[\"id\"] for img in coco_data[\"images\"])\n",
    "\n",
    "#randomly shuffle?\n",
    "split = int(82612*.8)\n",
    "\n",
    "training = list(set(img[\"image_id\"] for img in coco_data[\"annotations\"]))[:split]\n",
    "validation = list(set(img[\"image_id\"] for img in coco_data[\"annotations\"]))[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "#!pip install image_search\n",
    "import image_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataOrganizer:\n",
    "    def __init__(self, name):\n",
    "        filename = get_data_path(name)\n",
    "    \n",
    "        with Path(filename).open() as f:\n",
    "            captions_train = json.load(f)\n",
    "\n",
    "        image2url = {image[\"id\"]: image[\"coco_url\"] for image in captions_train[\"images\"]}\n",
    "    \n",
    "        image2captions = defaultdict(list)\n",
    "        for a in captions_train[\"annotations\"]:\n",
    "            image2captions[a[\"image_id\"]].append(a[\"id\"])\n",
    "    \n",
    "        self.caption2text = {a['id']: a['caption'] for a in captions_train['annotations']}\n",
    "        #self.descriptors = {descriptors['id']: descriptors\n",
    "    \n",
    "        image_id = captions_train['images'][0]['id']\n",
    "        image_url = image2url[image_id]\n",
    "\n",
    "    def get_caption(self, image_id):\n",
    "        #print(self.caption2text)\n",
    "        return self.caption2text.get(image_id)\n",
    "\n",
    "database = COCODataOrganizer(\"captions_train2014.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "skipped = []\n",
    "descriptors1 = []\n",
    "descriptors2 = []\n",
    "captions = []\n",
    "\n",
    "import random\n",
    "\n",
    "for id in training: \n",
    "     \n",
    "    confusor_image = None\n",
    "    \n",
    "    while confusor_image is None:\n",
    "        try:\n",
    "            confusor_image = resnet18_features[training[random.randrange(split - 1)]]\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    try:\n",
    "        if resnet18_features[id] is None:\n",
    "            continue\n",
    "        else:\n",
    "            #database.get_caption(id)\n",
    "            train.append([final_embeddings.get(id), resnet18_features[id], confusor_image])\n",
    "            descriptors1.append(resnet18_features[id])\n",
    "            descriptors2.append(confusor_image)\n",
    "            captions.append(final_embeddings.get(id))\n",
    "\n",
    "            if resnet18_features[id].shape != (1,512):\n",
    "                print(id)\n",
    "            if confusor_image.shape != (1,512):\n",
    "                print(confusor_image)\n",
    "   \n",
    "    except:\n",
    "        skipped.append(id)\n",
    "\n",
    "#print(len(skipped))\n",
    "#print(len(train))\n",
    "train = np.asarray(train, dtype = \"object\")\n",
    "#print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(database.get_caption(7975))\n",
    "#print(np.array(train[65954]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE\n",
    "import mygrad as mg\n",
    "\n",
    "model = Model()\n",
    "\n",
    "optim = SGD(model.parameters, learning_rate=0.1, momentum=0.9)  # \n",
    "batch_size = 32\n",
    "\n",
    "for epoch_cnt in range(1):\n",
    "    idxs = np.arange(65955) \n",
    "    np.random.shuffle(idxs)  \n",
    "    \n",
    "    for batch_cnt in range(0, 65955//batch_size):\n",
    "        batch_indices = idxs[batch_cnt*batch_size : (batch_cnt + 1)*batch_size]\n",
    "       \n",
    "        batch = np.asarray(descriptors1)[batch_indices.astype(int)]  # random batch of our training data\n",
    "        #print(batch.shape)\n",
    "        # `model.__call__ is responsible for performing the \"forward-pass\"\n",
    "        prediction = model(batch) \n",
    "        \n",
    "        #loss = margin_ranking_loss(descriptors1[batch_indices[0]], descriptors2[batch_indices[0]], captions[batch_indices[0]], 0.1)\n",
    "        loss = mg.mean(mg.maximum(0, 0.1 - np.asarray(captions)[batch_indices.astype(int)] * (prediction - model(np.asarray(descriptors2)[batch_indices.astype(int)]))))\n",
    "        # you still must compute all the gradients!\n",
    "        loss.backward()\n",
    "        \n",
    "        # the optimizer is responsible for updating all of the parameters\n",
    "        optim.step()\n",
    "        \n",
    "        plotter.set_train_batch({\"loss\" : loss.item()},\n",
    "                                 batch_size=batch_size)\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_database = {}\n",
    "for id in training:\n",
    "    try:\n",
    "        if resnet18_features[id] is not None:\n",
    "            descriptor = resnet18_features[id]\n",
    "            image_databse[id] = model(descriptor)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "def query(q, glove, idf, caption, k=5):\n",
    "    #weights = preprocess_captions([caption])\n",
    "    #print(caption)\n",
    "    #caption_embedding = embed_caption(weights, caption)\n",
    "    \n",
    "    #caption_embedding = glove[caption]\n",
    "    caption_embedding = embed_caption(q, glove, idf)\n",
    "    \n",
    "    similarities = []\n",
    "    for image_id, image_embedding in image_database.items():\n",
    "        similarity_score = np.dot(caption_embedding, image_embedding)\n",
    "        similarities.append((image_id, similarity_score))\n",
    "    \n",
    "    similarities.sort()\n",
    "    \n",
    "    return similarities[:k]\n",
    "    \n",
    "#print(query('sunny day'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queried = query(q,glove,idf, \"sunny day\")\n",
    "\n",
    "image2url = {image[\"id\"]: image[\"coco_url\"] for image in coco_data[\"images\"]}\n",
    "\n",
    "#queried = [57870, 384029, 222016, 520950]\n",
    "\n",
    "urls = [image2url.get(id) for id in queried]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://images.cocodataset.org/train2014/COCO_train2014_000000057870.jpg', 'http://images.cocodataset.org/train2014/COCO_train2014_000000384029.jpg', 'http://images.cocodataset.org/train2014/COCO_train2014_000000222016.jpg', 'http://images.cocodataset.org/train2014/COCO_train2014_000000520950.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
